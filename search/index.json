[{"content":"This is my first post. Nothing to see here\n","date":"2020-01-01T00:00:00Z","permalink":"https://Vinnie-Palazeti.github.io/p/first-post/","title":"First Post"},{"content":"I had the opportunity to research the City of Milwaukee\u0026rsquo;s housing market. The Assessor\u0026rsquo;s Office of Milwaukee provided the data. The research goals provided by the office were:\n  Identifying how market trends differentially impact communities of color and varying socioeconomic status; and\n  Determining whether Milwaukee property assessments change disproportionately according to demographic factors.\n  The data was 8500 home sales from 2018 to 2019, which included 60 house characteristics, e.g. Building Type, Number of Rooms, Kitchen Quality, Overall Condition, Square Footage, etc. Two of these measures, Sale Price \u0026amp; Assessment Price, comprise the variable of interest: The Assessment-to-Sale Ratio (ATS). This is the Assessment Price divided by the Sale price. The ATS ratio measures assessment precision. An ATS ratio below 1 indicates an underassessment, while a value above 1 is an overassessment. The mean ATS ratio is not of concern. If the average ATS ratio is 0.9, and observations are normally distributed around that value, then everyone is misassessed equally. This situation is fine. Issues arise when homes are misassessed in predictable ways. The Assessor\u0026rsquo;s Office wanted to know if demographic factors, specifically racial and/or economic, affected home assessment.\nEach observation had the Census Tract where the home resided, so The American Community Survey (ACS) had the demographic data I needed. The ACS has a wild number of variables, like 27,000 or so. Pulling this info from the Social Explorer website is annoying, so I used Python to query the API.\nCheck it out: https://github.com/Vinnie-Palazeti/Census/tree/main/CensusData.\nI adapted code I found online to most of the geographies the ACS API provides. With this script I pulled Hispanic, Black, and Not-Hispanic White Population, Median Income, Renter-Occupied Housing Units, Income-Below-Poverty Population, etc.\nI won\u0026rsquo;t bore you with the data cleaning. There was a lot.\nThe end goal is to employ multivariate regression techniques to assess the effect that population demographics have on the ATS ratio. Before I could jump into modeling, though, there were several obstacles.\nFirst, observations in regression must be Independent \u0026amp; Identically Distrbuted. My data breaks this rule. Homes within the same neighborhood are spatially correlated. Therefore, to capture the variance associated with this correlation I used a Hierarchical Linear Model. This is a fancy way of saying I coded Census Tract as a Random Effect, which is a fancy way of saying each Census Tract shares a common variance. With this method, I can determine the variance within and between Census Tracts, while also not violate an assumption of regression. Very good.\nNext, the ATS ratio is exactly that: a ratio. This breaks another rule of regression, namely that the outcome ought to be continuous. Many dispatch this rule, especially when dealing with percentages (ratios bounded [0,1]). This can lead to absurd predictions above or below the bounds. The ATS ratio is different; it is bounded by [0,∞]. This poses interesting issues.\nI found a 1933 article by Richard Kronmal discussing a toy dataset provided by Neyman. Kronmal says that regression coefficients in this situation are actually the joint effect, or interaction, of the coefficient and the denominator of the dependent variable. So, when regressing on the ATS ratio, each covariate in the model is actually an interaction term of Sale Price (denominator of ATS ratio). This is wild. Due to this complexity, I opted to run two sets of models. The first using the ATS ratio as the dependent variable, and the second using Assessment Price as the dependent variable and including Sale Price as a covariate. The latter structure seems more appropriate to me.\nThen, I noticed that for each Census Tract, the population of Black, White, and Hispanic people constituded roughly 95% of the population. In regression, covariates which are ratios with a common denomicator provides the same information. For example, each side of a percentage, say 0.6 and 0.4, relays the same information. Including both is not appropriate.\nThis is the situation with Milwaukee\u0026rsquo;s demographics. Where the population of Black people is 60%, the population of Whites is 25 to 35%, with Hispanics filling in most of the remaining gap. Therefore, including all three is no good. I opted to include only Black and Hispanic populations as independent variables, which are the specific variables of interest in this study.\nFinally, building high dimensional models has always felt like soft p hacking. Using 50 covariates and selectingly inferring on significant findings strays, in my opinion, so far from traditional experimental statistics that it renders the whole project null. Research decisions, even those made prior to data analysis, shape statistical findings. Therefore, I setup this analysis to only infer upon the independent variables of interest, namely Percent Black Population and Percent Hispanic Population. All other covariates are controls.\nI used HLMs to determine the effect of Percent Black and Hispanic population on assessment values. The models produced significant negative effects for both variables. The Simple vs Specific setup was used to determine if increasing the specificity of housing characteristics increased or decreased the effect of population demographics. I\u0026rsquo;ve seen this methodology used in observational studies, though the results, to my knowledge, can\u0026rsquo;t be quantified statistically. I believe the intuition is that including more precise housing characteristics ought to decrease the effect of population demographics, if the market valued the former over the latter. In the ATS ratio models I find that increasing home characteristics actually increases the effect of population demographics. However, the models with Assessment Value as the outcome variable produce mixed results.\nAll variables were scaled \u0026amp; centered prior to regressing. Therefore, a one standard deviation change in a coefficient is associated with a percent standard deviation change in the outcome. In table six we see -0.1218 as the coefficient for Percent Black in the Simple model. The interpretation is: a one standard deviation increase in the percent Black population is associated with a 0.12 standard deviation decrease in the assessment ratio. This is 12% of a standard deviation. The standard deviation of the assessment ratio, in our data, is about 0.13, so 12% of this is 0.016 or 1.16%. Therefore, observations in a neighborhood with a one standard deviation increase in the percent Black are subject to about a 1.16% decrease in their ATS Ratio.\nIn the Specific model we see this coefficient jump to −0.4642, which relates to a 6.14% decrease in their ATS Ratio. This is a significant decrease. When properties are over- or under-assessed differently, tax revenue is disproportionately raised. Our model reports that there is a decrease in assessment as the Percent Black or Hispanic rises, which results in lower taxation. Though, taxation is not the only concern; accessing credit could be made increasingly difficult through consistent underassessment.\nHere are the results from the models using Assessment Price as the dependent variable. Coefficients can be read in the same way as above.\nInterpretation should be done cautiously. Attempting to nail down the specific percentage effect of any single parameter is difficult in high dimensional modeling. Therefore, I opt to take a holistic overview. I observe a constant, negative, and significant impact from Percent Black and Percent Hispanic. I infer from these facts that a negative relationship exists between these parameters and their respective dependent variable.\nIf you want to slog through my full interpretation of all the results check out the paper here.\n","date":"2020-01-01T00:00:00Z","image":"https://Vinnie-Palazeti.github.io/rando.jpg","permalink":"https://Vinnie-Palazeti.github.io/p/milwaukee-assessment-project/","title":"Milwaukee Assessment Project"},{"content":"Prediction is cool, useful, and good. I, for some reason, am not impressed by it, especially with continuous outcomes. Every time I fit a Deep NN, XGBoost, LASSO, Random Forest, etc. I am left empty. My MSE is incredibly low. Hurray. Why is this the case? What variables are important? To what degree is each imput important?\nI understand the different goals of inference \u0026amp; prediction, but these methods are so powerful; they must be holding important information! I am interested in individual or joint inference on inputs to deep neural nets with continuous outcomes.\nThere are proposed inference methods. Guidotti (2018) provides a survey of the literature. He lays out four main definitions for \u0026ldquo;Black Box Model Explanation.\u0026rdquo; Three of which involve implementing a secondary model, outside of the neural net, that provides easier to understand rules for prediction. A decision tree is an example of such a secondary model. The last defintion, the one that interests me, he calls the Black Box Inspection Problem, which is defined as:\n Given a black box predictor b and a dataset D = {X, Y}, the black box inspection problem consists in finding a function f : (X → Y) × (X^n × Y^n) → V which takes as input a black box b and a dataset D, and returns a visual representation of the behavior of the black box, f(b, D) = v with V being the set of all possible representations.\n There are other methods, which construct test statistics using derivatives of node weights, but I am uninterested.\nThe Variable Effect Curve (VEC)\nKrause (2016) implements a VEC, overlayed with other graphics, for a classification problem. A VEC plots the Partial Dependence with fixed feature values on the x-axis, and the corresponding outcome probability on the y-axis.\nThere are two ways to find the probability at each input values. The formula recommended by Krause et al. is:\n Image 1 \nwhere pred is the prediction function, x_i is the input vector, v is the input value, and N is the number of rows in the design matrix. Every value for the feature in question is changed to v, then the prediction function is used on the new design matrix, and finally predictions are averaged. This process is iterated over a specific interval of v\u0026rsquo;s, usually the range of the feature.\nThe other method is to predict using the average level of all inputs other than a specific feature, which is iterated over its range. Both seem like valid approaches, while the latter could run into issues with dummy variables. If a covariate is either a 1 or 0, what does it mean to use an average, say 0.36, in prediction? I can\u0026rsquo;t think of a statistical reason of why this is flawed, but it doesn\u0026rsquo;t pass the intuition check.\nI implemented both versions of the Variable Effect Curve in python using the Wine Dataset. As noted above, I am interested in continuous outcomes. Therefore, this is not a classification problem, and (again) therefore I cannot use predicted probabilities. The outcome variable is wine quality, which has the range [3,8]. Instead of probabilites, I used the predicted outcome: wine quality score.\nThe covariate ranges are wildly different, so prior to modeling I scaled the predictors to [0,1]. Then, to predict over their \u0026ldquo;range\u0026rdquo;, I used 30 equally spaced values as inputs. I did this individually for each predictor. Here is the code for the former approach, which changes every value of a feature in the design matrix to one of the 30 \u0026ldquo;v\u0026rdquo; values\n VEC Code \nand the visual output (click image for higher quality)\n VEC Visual \nWhat I love about this is how incredibly interpretable it is. The positive effect alcohol \u0026amp; fixed acidity is clearly stated. Nice.\nHere is the code for predicting on the average value of each input, only iterively altering a single feature over the equal spaced range of v\n VEC Code \nand the visual\n VEC Visual \nThe general trends remain true, though the predictions with this formula are more irratic.\nAn issue with VECs used with deep nn models is the stocastic optimization function. Stocastic Gradient Descent or ADAM will usually not produce the same node weights in consecutive runs. This means that our graphs could look different, sometimes wildly different. Here is another VEC graph using the latter formula from above\n VEC Visual \nThis is no bueno. We see large shifts from the previous graph, especially at the ends. However, using the former formula, where we change every row of the design matrix to the value v, the outcomes are far more consistent\n VEC Visual \nAnother issue is the lack of any actual statistical tests. There are no distributions. Each value of x has a single prediction. There is a bootstrap opportunity here!\nVinnie\n","date":"2020-01-01T00:00:00Z","image":"https://Vinnie-Palazeti.github.io/p/variable-effect-curve/header_hu4551f5b7cd02f3d273d9e2d0a552bef1_14058_120x120_fill_q75_box_smart1.jpg","permalink":"https://Vinnie-Palazeti.github.io/p/variable-effect-curve/","title":"Variable Effect Curve"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.— Rob Pike1 Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Italics Bold Code     italics bold code    Code Blocks Code block with backticks \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Fruit  Apple Orange Banana   Dairy  Milk Cheese    Other Elements — abbr, sub, sup, kbd, mark GIFis a bitmap image format.\nH2O\nXn+ Yn= ZnPress CTRL+ALT+Deleteto end the session.\nMost salamandersare nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","date":"2019-03-11T00:00:00Z","image":"https://Vinnie-Palazeti.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://Vinnie-Palazeti.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n YouTube Privacy Enhanced Shortcode    Twitter Simple Shortcode .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”\n— Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019  Vimeo Simple Shortcode  .__h_video { position: relative; padding-bottom: 56.23%; height: 0; overflow: hidden; width: 100%; background: #000; } .__h_video img { width: 100%; height: auto; color: #000; } .__h_video .play { height: 72px; width: 72px; left: 50%; top: 50%; margin-left: -36px; margin-top: -36px; position: absolute; cursor: pointer; }  ","date":"2019-03-10T00:00:00Z","permalink":"https://Vinnie-Palazeti.github.io/p/rich-content/","title":"Rich Content"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial like so:  {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }}  To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files  Note: Use the online reference of Supported TeX Functions\nExamples Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","permalink":"https://Vinnie-Palazeti.github.io/p/math-typesetting/","title":"Math Typesetting"}]