[{"content":"Another inference post. I imagine there will be many more of these.\nWhat variables affect deep NN prediction? Sensitivity Analysis was purposed by Kewley et al. (2000) to answer this question. Here\u0026rsquo;s the gist:\nAfter fitting a model, in this case a deep NN, use the average of every feature for prediction. To test the model\u0026rsquo;s sensitivity to a specific feature, iterate over a range and make predictions at each point, while keeping all other features at their average level. Then, compute the mean squared difference of the predictions and the average sample target value.\nHere is how they describe it:\n Once a network has been trained on a large set of input variables, calculate an average value for each input variable. Then, holding all variables but one at a time at their average levels, vary the one input over its entire range and compute the variability produced in the net outputs. Analysis of this variability may be done for several different networks, each trained from a different weight initialization. The algorithm will then rank the variables from highest to lowest according to the mean variability produced in the output.\n The range of the feature is where I think this measure gets interesting. The range itself changes. If you scaled the feature [0,1], then any number of range levels is possible. There could be 3 levels (0.33,0.66,0.99) or 10 levels (0.1,0.2,0.3\u0026hellip;etc). The sensitivity statistic measures the prediction difference at different feature intervals. Therefore, models are \u0026ldquo;more sensitive\u0026rdquo; to features with higher sensitivity at the lower range intervals. Neat\nHere are some graphs:\n SA Visual \nThis is using the average levels of each feature for prediction. Like in the VEC analysis, these predictions can be wildly different from run to run.\n SA Visual \nVery different.\nThere is another method, which I used in the VEC post, that helps with this variability. Instead of using the average value, change the entire column of the data to the interval value and grab the average prediction.\n SA Visual \nThis method provides more consitent results.\n SA Visual \nThe authors go on to do significance testing of some sort, but I am not convinced of the efficacy. This is a cool method, but it doesn\u0026rsquo;t provide any information about in what way the feature is affecting the model. This is my primary interest.\nVinnie\n","date":"2020-01-07T00:00:00Z","image":"https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/header_hu830b1cb9ad185894eeea77a5a8bf77de_92822_120x120_fill_q75_box_smart1.jpg","permalink":"https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/","title":"Sensitivity Analysis"},{"content":"I had the opportunity to research the City of Milwaukee\u0026rsquo;s housing market. The Assessor\u0026rsquo;s Office of Milwaukee provided the data. The research goals provided by the office were:\n  Identifying how market trends differentially impact communities of color and varying socioeconomic status; and\n  Determining whether Milwaukee property assessments change disproportionately according to demographic factors.\n  The data was 8500 home sales from 2018 to 2019, which included 60 house characteristics, e.g. Building Type, Number of Rooms, Kitchen Quality, Overall Condition, Square Footage, etc. Two of these measures, Sale Price \u0026amp; Assessment Price, comprise the variable of interest: The Assessment-to-Sale Ratio (ATS). This is the Assessment Price divided by the Sale price. The ATS ratio measures assessment precision. An ATS ratio below 1 indicates an underassessment, while a value above 1 is an overassessment. The mean ATS ratio is not of concern. If the average ATS ratio is 0.9, and observations are normally distributed around that value, then everyone is misassessed equally. This situation is fine. Issues arise when homes are misassessed in predictable ways. The Assessor\u0026rsquo;s Office wanted to know if demographic factors, specifically racial and/or economic, affected home assessment.\nEach observation had the Census Tract where the home resided, so The American Community Survey (ACS) had the demographic data I needed. The ACS has a wild number of variables, like 27,000 or so. Pulling this info from the Social Explorer website is annoying, so I used Python to query the API.\nCheck it out: https://github.com/Vinnie-Palazeti/Census/tree/main/CensusData.\nI adapted code I found online to most of the geographies the ACS API provides. With this script I pulled Hispanic, Black, and Not-Hispanic White Population, Median Income, Renter-Occupied Housing Units, Income-Below-Poverty Population, etc.\nI won\u0026rsquo;t bore you with the data cleaning. There was a lot.\nThe end goal is to employ multivariate regression techniques to assess the effect that population demographics have on the ATS ratio. Before I could jump into modeling, though, there were several obstacles.\nFirst, observations in regression must be Independent \u0026amp; Identically Distrbuted. My data breaks this rule. Homes within the same neighborhood are spatially correlated. Therefore, to capture the variance associated with this correlation I used a Hierarchical Linear Model. This is a fancy way of saying I coded Census Tract as a Random Effect, which is a fancy way of saying each Census Tract shares a common variance. With this method, I can determine the variance within and between Census Tracts, while also not violate an assumption of regression. Very good.\nNext, the ATS ratio is exactly that: a ratio. This breaks another rule of regression, namely that the outcome ought to be continuous. Many dispatch this rule, especially when dealing with percentages (ratios bounded [0,1]). This can lead to absurd predictions above or below the bounds. The ATS ratio is different; it is bounded by [0,∞]. This poses interesting issues.\nI found a 1933 article by Richard Kronmal discussing a toy dataset provided by Neyman. Kronmal says that regression coefficients in this situation are actually the joint effect, or interaction, of the coefficient and the denominator of the dependent variable. So, when regressing on the ATS ratio, each covariate in the model is actually an interaction term of Sale Price (denominator of ATS ratio). This is wild. Due to this complexity, I opted to run two sets of models. The first using the ATS ratio as the dependent variable, and the second using Assessment Price as the dependent variable and including Sale Price as a covariate. The latter structure seems more appropriate to me.\nThen, I noticed that for each Census Tract, the population of Black, White, and Hispanic people constituded roughly 95% of the population. In regression, covariates which are ratios with a common denomicator provides the same information. For example, each side of a percentage, say 0.6 and 0.4, relays the same information. Including both is not appropriate.\nThis is the situation with Milwaukee\u0026rsquo;s demographics. Where the population of Black people is 60%, the population of Whites is 25 to 35%, with Hispanics filling in most of the remaining gap. Therefore, including all three is no good. I opted to include only Black and Hispanic populations as independent variables, which are the specific variables of interest in this study.\nFinally, building high dimensional models has always felt like soft p hacking. Using 50 covariates and selectingly inferring on significant findings strays, in my opinion, so far from traditional experimental statistics that it renders the whole project null. Research decisions, even those made prior to data analysis, shape statistical findings. Therefore, I setup this analysis to only infer upon the independent variables of interest, namely Percent Black Population and Percent Hispanic Population. All other covariates are controls.\nI used HLMs to determine the effect of Percent Black and Hispanic population on assessment values. The models produced significant negative effects for both variables. The Simple vs Specific setup was used to determine if increasing the specificity of housing characteristics increased or decreased the effect of population demographics. I\u0026rsquo;ve seen this methodology used in observational studies, though the results, to my knowledge, can\u0026rsquo;t be quantified statistically. I believe the intuition is that including more precise housing characteristics ought to decrease the effect of population demographics, if the market valued the former over the latter. In the ATS ratio models I find that increasing home characteristics actually increases the effect of population demographics. However, the models with Assessment Value as the outcome variable produce mixed results.\n ATS Ratio Model \nAll variables were scaled \u0026amp; centered prior to regressing. Therefore, a one standard deviation change in a coefficient is associated with a percent standard deviation change in the outcome. In table six we see -0.1218 as the coefficient for Percent Black in the Simple model. The interpretation is: a one standard deviation increase in the percent Black population is associated with a 0.12 standard deviation decrease in the assessment ratio. This is 12% of a standard deviation. The standard deviation of the assessment ratio, in our data, is about 0.13, so 12% of this is 0.016 or 1.16%. Therefore, observations in a neighborhood with a one standard deviation increase in the percent Black are subject to about a 1.16% decrease in their ATS Ratio.\nIn the Specific model we see this coefficient jump to −0.4642, which relates to a 6.14% decrease in their ATS Ratio. This is a significant decrease. When properties are over- or under-assessed differently, tax revenue is disproportionately raised. Our model reports that there is a decrease in assessment as the Percent Black or Hispanic rises, which results in lower taxation. Though, taxation is not the only concern; accessing credit could be made increasingly difficult through consistent underassessment.\nHere are the results from the models using Assessment Price as the dependent variable. Coefficients can be read in the same way as above.\n Assessment Price Model \nInterpretation should be done cautiously. Attempting to nail down the specific percentage effect of any single parameter is difficult in high dimensional modeling. Therefore, I opt to take a holistic overview. I observe a constant, negative, and significant impact from Percent Black and Percent Hispanic. I infer from these facts that a negative relationship exists between these parameters and their respective dependent variable.\nIf you want to slog through my full interpretation of all the results check out the paper here.\n","date":"2020-01-01T00:00:00Z","image":"https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/rando_hu65ac93192e7f579550509eaf1e1bf388_129059_120x120_fill_q75_box_smart1.jpg","permalink":"https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/","title":"Milwaukee Assessment"},{"content":"Prediction is cool, useful, and good. I, for some reason, am not impressed by it, especially with continuous outcomes. Every time I fit a Deep NN, XGBoost, LASSO, Random Forest, etc. I am left empty. My MSE is incredibly low. Hurray. Why is this the case? What variables are important? To what degree is each imput important?\nI understand the different goals of inference \u0026amp; prediction, but these methods are so powerful; they must be holding important information! I am interested in individual or joint inference on inputs to deep neural nets with continuous outcomes.\nThere are proposed inference methods. Guidotti (2018) provides a survey of the literature. He lays out four main definitions for \u0026ldquo;Black Box Model Explanation.\u0026rdquo; Three of which involve implementing a secondary model, outside of the neural net, that provides easier to understand rules for prediction. A decision tree is an example of such a secondary model. The last defintion, the one that interests me, he calls the Black Box Inspection Problem, which is defined as:\n Given a black box predictor b and a dataset D = {X, Y}, the black box inspection problem consists in finding a function f : (X → Y) × (X^n × Y^n) → V which takes as input a black box b and a dataset D, and returns a visual representation of the behavior of the black box, f(b, D) = v with V being the set of all possible representations.\n There are other methods, which construct test statistics using derivatives of node weights, but I am uninterested.\nThe Variable Effect Curve (VEC)\nKrause (2016) implements a VEC, overlayed with other graphics, for a classification problem. A VEC plots the Partial Dependence with fixed feature values on the x-axis, and the corresponding outcome probability on the y-axis.\nThere are two ways to find the probability at each input values. The formula recommended by Krause et al. is:\n Image 1 \nwhere pred is the prediction function, x_i is the input vector, v is the input value, and N is the number of rows in the design matrix. Every value for the feature in question is changed to v, then the prediction function is used on the new design matrix, and finally predictions are averaged. This process is iterated over a specific interval of v\u0026rsquo;s, usually the range of the feature.\nThe other method is to predict using the average level of all inputs other than a specific feature, which is iterated over its range. Both seem like valid approaches, while the latter could run into issues with dummy variables. If a covariate is either a 1 or 0, what does it mean to use an average, say 0.36, in prediction? I can\u0026rsquo;t think of a statistical reason of why this is flawed, but it doesn\u0026rsquo;t pass the intuition check.\nI implemented both versions of the Variable Effect Curve in python using the Wine Dataset. As noted above, I am interested in continuous outcomes. Therefore, this is not a classification problem, and (again) therefore I cannot use predicted probabilities. The outcome variable is wine quality, which has the range [3,8]. Instead of probabilites, I used the predicted outcome: wine quality score.\nThe covariate ranges are wildly different, so prior to modeling I scaled the predictors to [0,1]. Then, to predict over their \u0026ldquo;range\u0026rdquo;, I used 30 equally spaced values as inputs. I did this individually for each predictor. Here is the code for the former approach, which changes every value of a feature in the design matrix to one of the 30 \u0026ldquo;v\u0026rdquo; values\n VEC Code \nand the visual output\n VEC Visual \nWhat I love about this is how incredibly interpretable it is. The positive effect alcohol \u0026amp; fixed acidity is clearly stated. Nice.\nHere is the code for predicting on the average value of each input, only iterively altering a single feature over the equal spaced range of v\n VEC Code \nand the visual\n VEC Visual \nThe general trends remain true, though the predictions with this formula are more irratic.\nAn issue with VECs used with deep nn models is the stocastic optimization function. Stocastic Gradient Descent or ADAM will usually not produce the same node weights in consecutive runs. This means that our graphs could look different, sometimes wildly different. Here is another VEC graph using the latter formula from above\n VEC Visual \nThis is no bueno. We see large shifts from the previous graph, especially at the ends. However, using the former formula, where we change every row of the design matrix to the value v, the outcomes are far more consistent\n VEC Visual \nAnother issue is the lack of any actual statistical tests. There are no distributions. Each value of x has a single prediction. There is a bootstrap opportunity here!\nVinnie\n","date":"2020-01-01T00:00:00Z","image":"https://Vinnie-Palazeti.github.io/p/variable-effect-curve/header_hu4551f5b7cd02f3d273d9e2d0a552bef1_14058_120x120_fill_q75_box_smart1.jpg","permalink":"https://Vinnie-Palazeti.github.io/p/variable-effect-curve/","title":"Variable Effect Curve"}]