<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Vincenzo Palazeti</title>
        <link>https://Vinnie-Palazeti.github.io/</link>
        <description>Recent content on Vincenzo Palazeti</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 07 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://Vinnie-Palazeti.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Sensitivity Analysis</title>
        <link>https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/</link>
        <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/</guid>
        <description>&lt;img src="https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/header.jpg" alt="Featured image of post Sensitivity Analysis" /&gt;&lt;p&gt;Another inference post. I imagine there will be many more of these.&lt;/p&gt;
&lt;p&gt;What variables affect deep NN prediction? Sensitivity Analysis was purposed by Kewley et al. (2000) to answer this question. Here&amp;rsquo;s the gist:&lt;/p&gt;
&lt;p&gt;After fitting a model, in this case a deep NN, use the average of every feature for prediction. To test the model&amp;rsquo;s sensitivity to a specific feature, iterate over a range and make predictions at each point, while keeping all other features at their average level. Then, compute the mean squared difference of the predictions and the average sample target value.&lt;/p&gt;
&lt;p&gt;Here is how they describe it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once a network has been trained on a large set of input variables, calculate an average value for each input variable. Then, holding all variables but one at a time at their average levels, vary the one input over its entire range and compute the variability produced in the net outputs. Analysis of this variability may be done for several different networks, each trained from a different weight initialization. The algorithm will then rank the variables from highest to lowest according to the mean variability produced in the output.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The range of the feature is where I think this measure gets interesting. The range itself changes. If you scaled the feature [0,1], then any number of range levels is possible. There could be 3 levels (0.33,0.66,0.99) or 10 levels (0.1,0.2,0.3&amp;hellip;etc). The sensitivity statistic measures the prediction difference at different feature intervals. Therefore, models are &amp;ldquo;more sensitive&amp;rdquo; to features with higher sensitivity at the lower range intervals. Neat&lt;/p&gt;
&lt;p&gt;Here are some graphs:&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 150; flex-basis: 360px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals1.jpg&#34; data-size=&#34;1800x1200&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals1.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals1_hu6a9d1f823f3493ad8582977aab8e8624_221285_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals1_hu6a9d1f823f3493ad8582977aab8e8624_221285_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1800&#34;
				height=&#34;1200&#34;
				loading=&#34;lazy&#34;
				alt=&#34;SA Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;SA Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;This is using the average levels of each feature for prediction. Like in the VEC analysis, these predictions can be wildly different from run to run.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 150; flex-basis: 360px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals2.jpg&#34; data-size=&#34;1800x1200&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals2.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals2_hu6a9d1f823f3493ad8582977aab8e8624_213925_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Using_Average_Vals2_hu6a9d1f823f3493ad8582977aab8e8624_213925_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1800&#34;
				height=&#34;1200&#34;
				loading=&#34;lazy&#34;
				alt=&#34;SA Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;SA Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Very different.&lt;/p&gt;
&lt;p&gt;There is another method, which I used in the VEC post, that helps with this variability. Instead of using the average value, change the entire column of the data to the interval value and grab the average prediction.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 150; flex-basis: 360px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col1.jpg&#34; data-size=&#34;1800x1200&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col1.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col1_hu6a9d1f823f3493ad8582977aab8e8624_232952_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col1_hu6a9d1f823f3493ad8582977aab8e8624_232952_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1800&#34;
				height=&#34;1200&#34;
				loading=&#34;lazy&#34;
				alt=&#34;SA Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;SA Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;This method provides more consitent results.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 150; flex-basis: 360px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col2.jpg&#34; data-size=&#34;1800x1200&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col2.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col2_hu6a9d1f823f3493ad8582977aab8e8624_224426_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/sensitivity-analysis/SA_Change_Whole_Col2_hu6a9d1f823f3493ad8582977aab8e8624_224426_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1800&#34;
				height=&#34;1200&#34;
				loading=&#34;lazy&#34;
				alt=&#34;SA Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;SA Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;The authors go on to do significance testing of some sort, but I am not convinced of the efficacy. This is a cool method, but it doesn&amp;rsquo;t provide any information about in what way the feature is affecting the model. This is my primary interest.&lt;/p&gt;
&lt;p&gt;Vinnie&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Milwaukee Assessment</title>
        <link>https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/</link>
        <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/</guid>
        <description>&lt;img src="https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/rando.jpg" alt="Featured image of post Milwaukee Assessment" /&gt;&lt;p&gt;I had the opportunity to research the City of Milwaukee&amp;rsquo;s housing market. The Assessor&amp;rsquo;s Office of Milwaukee provided the data. The research goals provided by the office were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Identifying how market trends differentially impact communities of color and varying
socioeconomic status; and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Determining whether Milwaukee property assessments change disproportionately
according to demographic factors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data was 8500 home sales from 2018 to 2019, which included 60 house characteristics, e.g. Building Type, Number of Rooms, Kitchen Quality, Overall Condition, Square Footage, etc. Two of these measures, Sale Price &amp;amp; Assessment Price, comprise the variable of interest: The Assessment-to-Sale Ratio (ATS). This is the Assessment Price divided by the Sale price. The ATS ratio measures assessment precision. An ATS ratio below 1 indicates an underassessment, while a value above 1 is an overassessment. The mean ATS ratio is not of concern. If the average ATS ratio is 0.9, and observations are normally distributed around that value, then everyone is misassessed equally. This situation is fine. Issues arise when homes are misassessed in predictable ways. The Assessor&amp;rsquo;s Office wanted to know if demographic factors, specifically racial and/or economic, affected home assessment.&lt;/p&gt;
&lt;p&gt;Each observation had the Census Tract where the home resided, so The American Community Survey (ACS) had the demographic data I needed. The ACS has a wild number of variables, like 27,000 or so. Pulling this info from the Social Explorer website is annoying, so I used Python to query the API.&lt;/p&gt;
&lt;p&gt;Check it out: &lt;a href=&#34;https://github.com/Vinnie-Palazeti/Census/tree/main/CensusData&#34;&gt;https://github.com/Vinnie-Palazeti/Census/tree/main/CensusData&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I adapted code I found online to most of the geographies the ACS API provides. With this script I pulled Hispanic, Black, and Not-Hispanic White Population, Median Income, Renter-Occupied Housing Units, Income-Below-Poverty Population, etc.&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t bore you with the data cleaning. There was a lot.&lt;/p&gt;
&lt;p&gt;The end goal is to employ multivariate regression techniques to assess the effect that population demographics have on the ATS ratio. Before I could jump into modeling, though, there were several obstacles.&lt;/p&gt;
&lt;p&gt;First, observations in regression must be Independent &amp;amp; Identically Distrbuted. My data breaks this rule. Homes within the same neighborhood are spatially correlated. Therefore, to capture the variance associated with this correlation I used a Hierarchical Linear Model. This is a fancy way of saying I coded Census Tract as a Random Effect, which is a fancy way of saying each Census Tract shares a common variance. With this method, I can determine the variance &lt;em&gt;within&lt;/em&gt; and &lt;em&gt;between&lt;/em&gt; Census Tracts, while also not violate an assumption of regression. Very good.&lt;/p&gt;
&lt;p&gt;Next, the ATS ratio is exactly that: a ratio. This breaks another rule of regression, namely that the outcome ought to be continuous. Many dispatch this rule, especially when dealing with percentages (ratios bounded [0,1]). This can lead to absurd predictions above or below the bounds. The ATS ratio is different; it is bounded by [0,∞]. This poses interesting issues.&lt;/p&gt;
&lt;p&gt;I found a 1933 article by Richard Kronmal discussing a toy dataset provided by Neyman. Kronmal says that regression coefficients in this situation are actually the joint effect, or interaction, of the coefficient and the denominator of the dependent variable. So, when regressing on the ATS ratio, each covariate in the model is actually an interaction term of Sale Price (denominator of ATS ratio). This is wild. Due to this complexity, I opted to run two sets of models. The first using the ATS ratio as the dependent variable, and the second using Assessment Price as the dependent variable and including Sale Price as a covariate. The latter structure seems more appropriate to me.&lt;/p&gt;
&lt;p&gt;Then, I noticed that for each Census Tract, the population of Black, White, and Hispanic people constituded roughly 95% of the population. In regression, covariates which are ratios with a common denomicator provides the same information. For example, each side of a percentage, say 0.6 and 0.4, relays the same information. Including both is not appropriate.&lt;/p&gt;
&lt;p&gt;This is the situation with Milwaukee&amp;rsquo;s demographics. Where the population of Black people is 60%, the population of Whites is 25 to 35%, with Hispanics filling in most of the remaining gap. Therefore, including all three is no good. I opted to include only Black and Hispanic populations as independent variables, which are the specific variables of interest in this study.&lt;/p&gt;
&lt;p&gt;Finally, building high dimensional models has always felt like soft p hacking. Using 50 covariates and selectingly inferring on significant findings strays, in my opinion, so far from traditional experimental statistics that it renders the whole project null. Research decisions, even those made &lt;strong&gt;prior&lt;/strong&gt; to data analysis, &lt;a class=&#34;link&#34; href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.694.7217&amp;amp;rep=rep1&amp;amp;type=pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;shape statistical findings.&lt;/a&gt; Therefore, I setup this analysis to only infer upon the independent variables of interest, namely Percent Black Population and Percent Hispanic Population. All other covariates are controls.&lt;/p&gt;
&lt;p&gt;I used HLMs to determine the effect of Percent Black and Hispanic population on assessment values. The models produced significant negative effects for both variables. The Simple vs Specific setup was used to determine if increasing the specificity of housing characteristics increased or decreased the effect of population demographics. I&amp;rsquo;ve seen this methodology used in observational studies, though the results, to my knowledge, can&amp;rsquo;t be quantified statistically. I believe the intuition is that including more precise housing characteristics ought to decrease the effect of population demographics, if the market valued the former over the latter. In the ATS ratio models I find that increasing home characteristics actually &lt;strong&gt;increases&lt;/strong&gt; the effect of population demographics. However, the models with Assessment Value as the outcome variable produce mixed results.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 328; flex-basis: 788px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/ATS_Ratio.jpg&#34; data-size=&#34;2266x690&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/ATS_Ratio.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/ATS_Ratio_hu55b27eb1372cc136b2be4848ecb51f02_120725_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/ATS_Ratio_hu55b27eb1372cc136b2be4848ecb51f02_120725_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;2266&#34;
				height=&#34;690&#34;
				loading=&#34;lazy&#34;
				alt=&#34;ATS Ratio Model&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;ATS Ratio Model&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;All variables were scaled &amp;amp; centered prior to regressing. Therefore, a one standard deviation change in a coefficient is associated with a percent standard deviation change in the outcome. In table six we see -0.1218 as the coefficient for Percent Black in the Simple model. The interpretation is: a one standard deviation increase in the percent Black population is associated with a 0.12 standard deviation decrease in the assessment ratio. This is 12% of a standard deviation. The standard deviation of the assessment ratio, in our data, is about 0.13, so 12% of this is 0.016 or 1.16%. Therefore, observations in a neighborhood with a one standard deviation increase in the percent Black are subject to about a 1.16% decrease in their ATS Ratio.&lt;/p&gt;
&lt;p&gt;In the Specific model we see this coefficient jump to −0.4642, which relates to a 6.14% decrease in their ATS Ratio. This is a significant decrease. When properties are over- or under-assessed differently, tax revenue is disproportionately raised. Our model reports that there is a decrease in assessment as the Percent Black or Hispanic rises, which results in lower taxation. Though, taxation is not the only concern; accessing credit could be made increasingly difficult through consistent underassessment.&lt;/p&gt;
&lt;p&gt;Here are the results from the models using Assessment Price as the dependent variable. Coefficients can be read in the same way as above.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 337; flex-basis: 809px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/Assess_Price.jpg&#34; data-size=&#34;2246x666&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/Assess_Price.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/Assess_Price_hubcfa795814c9c6954ab012fea21a9475_115227_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/milwaukee-assessment/Assess_Price_hubcfa795814c9c6954ab012fea21a9475_115227_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;2246&#34;
				height=&#34;666&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Assessment Price Model&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Assessment Price Model&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Interpretation should be done cautiously. Attempting to nail down the specific percentage effect of any single parameter is difficult in high dimensional modeling. Therefore, I opt to take a holistic overview. I observe a constant, negative, and significant impact from Percent Black and Percent Hispanic. I infer from these facts that a negative relationship exists between these parameters and their respective dependent variable.&lt;/p&gt;
&lt;p&gt;If you want to slog through my full interpretation of all the results check out the paper &lt;a class=&#34;link&#34; href=&#34;https://github.com/Vinnie-Palazeti/Milwaukee-Property-Assessment/blob/master/Final%20Paper%20%26%20Presentation/Milwaukee%20Final%20Report.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here.&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Variable Effect Curve</title>
        <link>https://Vinnie-Palazeti.github.io/p/variable-effect-curve/</link>
        <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://Vinnie-Palazeti.github.io/p/variable-effect-curve/</guid>
        <description>&lt;img src="https://Vinnie-Palazeti.github.io/p/variable-effect-curve/header.jpg" alt="Featured image of post Variable Effect Curve" /&gt;&lt;p&gt;Prediction is cool, useful, and good. I, for some reason, am not impressed by it, especially with continuous outcomes. Every time I fit a Deep NN, XGBoost, LASSO, Random Forest, etc. I am left empty. My MSE is incredibly low. Hurray. Why is this the case? What variables are important? To what degree is each imput important?&lt;/p&gt;
&lt;p&gt;I understand the different goals of inference &amp;amp; prediction, but these methods are so powerful; they must be holding important information! I am interested in individual or joint inference on inputs to deep neural nets with continuous outcomes.&lt;/p&gt;
&lt;p&gt;There are proposed inference methods. Guidotti (2018) provides a survey of the literature. He lays out four main definitions for &amp;ldquo;Black Box Model Explanation.&amp;rdquo; Three of which involve implementing a secondary model, outside of the neural net, that provides easier to understand rules for prediction. A decision tree is an example of such a secondary model. The last defintion, the one that interests me, he calls the Black Box Inspection Problem, which is defined as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a black box predictor b and a dataset D = {X, Y}, the black box inspection problem consists in finding a function f : (X → Y) × (X^n × Y^n) → V which takes as input a black box b and a dataset D, and returns a visual representation of the behavior of the black box, f(b, D) = v with V being the set of all possible representations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are other methods, which construct test statistics using derivatives of node weights, but I am uninterested.&lt;/p&gt;
&lt;p&gt;The Variable Effect Curve (VEC)&lt;/p&gt;
&lt;p&gt;Krause (2016) implements a VEC, overlayed with other graphics, for a classification problem. A VEC plots the Partial Dependence with fixed feature values on the x-axis, and the corresponding outcome probability on the y-axis.&lt;/p&gt;
&lt;p&gt;There are two ways to find the probability at each input values. The formula recommended by Krause et al. is:&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 455; flex-basis: 1093px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/Partial_Dependence_Form.jpg&#34; data-size=&#34;747x164&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/Partial_Dependence_Form.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/Partial_Dependence_Form_hu2e79da07014f6d8a2800f41fd2ddab35_20753_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/Partial_Dependence_Form_hu2e79da07014f6d8a2800f41fd2ddab35_20753_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;747&#34;
				height=&#34;164&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Image 1&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Image 1&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;pred&lt;/em&gt; is the prediction function, x_i is the input vector, v is the input value, and N is the number of rows in the design matrix. Every value for the feature in question is changed to v, then the prediction function is used on the new design matrix, and finally predictions are averaged. This process is iterated over a specific interval of v&amp;rsquo;s, usually the range of the feature.&lt;/p&gt;
&lt;p&gt;The other method is to predict using the average level of all inputs other than a specific feature, which is iterated over its range. Both seem like valid approaches, while the latter could run into issues with dummy variables. If a covariate is either a 1 or 0, what does it mean to use an average, say 0.36, in prediction? I can&amp;rsquo;t think of a statistical reason of why this is flawed, but it doesn&amp;rsquo;t pass the intuition check.&lt;/p&gt;
&lt;p&gt;I implemented both versions of the Variable Effect Curve in python using the Wine Dataset. As noted above, I am interested in continuous outcomes. Therefore, this is not a classification problem, and (again) therefore I cannot use predicted probabilities. The outcome variable is wine quality, which has the range [3,8]. Instead of probabilites, I used the predicted outcome: wine quality score.&lt;/p&gt;
&lt;p&gt;The covariate ranges are wildly different, so prior to modeling I scaled the predictors to [0,1]. Then, to predict over their &amp;ldquo;range&amp;rdquo;, I used 30 equally spaced values as inputs. I did this individually for each predictor. Here is the code for the former approach, which changes every value of a feature in the design matrix to one of the 30 &amp;ldquo;v&amp;rdquo; values&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 362; flex-basis: 869px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_change_every_row.jpg&#34; data-size=&#34;1116x308&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_change_every_row.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_change_every_row_hu9833381f58faf51f2669f8d7c7c953cf_71568_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_change_every_row_hu9833381f58faf51f2669f8d7c7c953cf_71568_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1116&#34;
				height=&#34;308&#34;
				loading=&#34;lazy&#34;
				alt=&#34;VEC Code&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;VEC Code&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;and the visual output&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 100; flex-basis: 240px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col1.jpg&#34; data-size=&#34;2400x2400&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col1.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col1_hu6a9d1f823f3493ad8582977aab8e8624_452310_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col1_hu6a9d1f823f3493ad8582977aab8e8624_452310_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;2400&#34;
				height=&#34;2400&#34;
				loading=&#34;lazy&#34;
				alt=&#34;VEC Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;VEC Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;What I love about this is how incredibly interpretable it is. The positive effect alcohol &amp;amp; fixed acidity is clearly stated. Nice.&lt;/p&gt;
&lt;p&gt;Here is the code for predicting on the average value of each input, only iterively altering a single feature over the equal spaced range of v&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 336; flex-basis: 807px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_AVERAGE_CODE.jpg&#34; data-size=&#34;1104x328&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_AVERAGE_CODE.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_AVERAGE_CODE_hu8e95bfb9e34d7a0a24d07bdb28395c12_76781_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_AVERAGE_CODE_hu8e95bfb9e34d7a0a24d07bdb28395c12_76781_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1104&#34;
				height=&#34;328&#34;
				loading=&#34;lazy&#34;
				alt=&#34;VEC Code&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;VEC Code&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;and the visual&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 100; flex-basis: 240px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals1.jpg&#34; data-size=&#34;2400x2400&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals1.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals1_hu6a9d1f823f3493ad8582977aab8e8624_449452_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals1_hu6a9d1f823f3493ad8582977aab8e8624_449452_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;2400&#34;
				height=&#34;2400&#34;
				loading=&#34;lazy&#34;
				alt=&#34;VEC Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;VEC Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;The general trends remain true, though the predictions with this formula are more irratic.&lt;/p&gt;
&lt;p&gt;An issue with VECs used with deep nn models is the stocastic optimization function. Stocastic Gradient Descent or ADAM will usually not produce the same node weights in consecutive runs. This means that our graphs could look different, sometimes wildly different. Here is another VEC graph using the latter formula from above&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 100; flex-basis: 240px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals2.jpg&#34; data-size=&#34;2400x2400&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals2.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals2_hu6a9d1f823f3493ad8582977aab8e8624_443990_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Using_Average_Vals2_hu6a9d1f823f3493ad8582977aab8e8624_443990_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;2400&#34;
				height=&#34;2400&#34;
				loading=&#34;lazy&#34;
				alt=&#34;VEC Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;VEC Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;This is no bueno. We see large shifts from the previous graph, especially at the ends. However, using the former formula, where we change every row of the design matrix to the value v, the outcomes are far more consistent&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 100; flex-basis: 240px&#34;&gt;
		&lt;a href=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col2.jpg&#34; data-size=&#34;2400x2400&#34;&gt;&lt;img src=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col2.jpg&#34;
				srcset=&#34;https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col2_hu6a9d1f823f3493ad8582977aab8e8624_416121_480x0_resize_q75_box.jpg 480w, https://Vinnie-Palazeti.github.io/p/variable-effect-curve/VEC_Change_Whole_Col2_hu6a9d1f823f3493ad8582977aab8e8624_416121_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;2400&#34;
				height=&#34;2400&#34;
				loading=&#34;lazy&#34;
				alt=&#34;VEC Visual&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;VEC Visual&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Another issue is the lack of any actual statistical tests. There are no distributions. Each value of x has a single prediction. There is a bootstrap opportunity here!&lt;/p&gt;
&lt;p&gt;Vinnie&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
